# Метрика качества

## Подзадача 1 – Text QA

Для оценки качества текстовых заданий вида "вопрос-ответ" используется метрика **F1**.
Метрика **F1** рассчитывается по всем токенам в предсказанном (после постобработки сгенерированного текста) и истинном ответах и выражает среднее гармоническое точности (precision) и полноты (recall) предсказанных ответов.
Стандартная формула для метрики: 

![image](https://latex.codecogs.com/svg.image?S_{TextQA}=F1&space;=&space;2\cdot&space;\frac{Recall&space;\cdot&space;Precision}{Recall&space;&plus;&space;Precision})

Для оценки ответов на вопросы (**TextQA**): 

![image](https://latex.codecogs.com/svg.image?\text{precision}&space;=&space;\frac{\char"0023&space;tokens_{common}}{\char"0023tokens_{predicted}},&space;\text{recall}&space;=&space;\frac{\char"0023&space;tokens_{common}}{\char"0023&space;tokens_{gt}},)

где ![image](https://latex.codecogs.com/svg.image?\char"0023&space;tokens_{common}) — количество общих токенов для предсказанного и истинного ответа;
![image](https://latex.codecogs.com/svg.image?\char"0023&space;tokens_{predicted})— количество токенов в предсказанном ответе;
![image](https://latex.codecogs.com/svg.image?\char"0023&space;tokens_{gt}) — количество токенов в истинном ответе.

## Подзадача 2 – Mathematical QA

Для оценки качества математических заданий вида "вопрос-ответ" используется метрика **Exact Match (EM)** $(S_{MathQA})$.

EM принимает значение 1, если все токены в предсказанном ответе (после постобработки сгенерированного текста) полностью совпадают с токенами в истинном ответе, в противном случае значение метрики $EM = 0$.

Данная метрика используется для математических задач с множественным выбором (**MathQA**), где ответом может выступать либо только буква, соответствующая правильному варианту ответа из предложенных, либо число, соответствующее правильному решению математической задачи, либо комбинация правильной буквы и числа.

## Подзадача 3 – Image Generation

Для оценки качества решения задач text-to-image используются две основные метрики: **FID** и **CLIP**. Данные метрики широко используются в задачах генерации изображений по тексту.

**FID** — это расстояние Фреше между двумя многомерными распределениями Гаусса: $N(\mu,\Sigma)$ – распределением признаков изображений, созданных генератором, и $N(\mu_{\omega},\Sigma_{\omega})$ – распределением признаков реальных изображений, используемых для обучения сети. В качестве нейронной сети для получения признаков обычно используется Inception v3, обученный на ImageNet. В результате метрику можно вычислить по средним значениям и ковариации активаций, когда синтезированные и реальные изображения подаются в сеть Inception:

![image](https://latex.codecogs.com/svg.image?\text{FID}=|\mu&space;-&space;\mu_\omega|^{2}&plus;tr(\Sigma&space;&plus;&space;\Sigma_\omega-2(\Sigma\Sigma_\omega)^{\frac{1}{2}}).)

Вместо прямого сравнения изображений пиксель за пикселем, FID сравнивает среднее значение и стандартное отклонение одного из более глубоких слоев в сверточной нейронной сети. Эти слои расположены ближе к выходным узлам, которые соответствуют объектам реального мира, таким как определенная порода собак или самолет, и дальше от мелких слоев рядом с входным изображением. В результате они имеют тенденцию имитировать человеческое восприятие сходства изображений. Значение метрики тем лучше, чем оно ближе к 0, при этом метрика ненормированная и значения сильно зависят от объема выборки, используемой для ее измерения.
 
**CLIP score** – метрика, которая позволяет оценить, насколько визуальное представление соответствует текстовому описанию. Для расчета метрики используется нейронная сеть CLIP, которая для каждой пары "картинка-текст" выдает соответствующие текстовый и визуальный эмбеддинги. Полученные репрезентации сравниваются по косинусному расстоянию (cosine similarity). Cosine similarity отражает меру сходства между двумя векторами и вычисляется по формуле:

![image](https://latex.codecogs.com/svg.image?\text{similarity}=\frac{x_1&space;x_2}{max(\left\|&space;x_1\right\|_2&space;\boldsymbol{\cdot}&space;\left\|&space;x_2\right\|_2,\epsilon)},)

где $x_1$ - текстовый embedding, $x_2$ - картиночный эмбеддинг, $\epsilon$ - положительное сколь угодно малое число, вводимое для избежания деления на 0.

Косинусное расстояние равно 1, если соответствующие текстовые и визуальные векторные репрезентации совпадают – и 0, если полностью отличны друг от друга. Общее значение CLIP score вычисляется как усредненное значение метрики, рассчитанное по всем тестовым примерам для этой задачи.

Финальная метрика для оценки задачи text-to-image представляет собой комбинацию метрик **FID** и **CLIP score** и рассчитывается по следующей формуле:

![image](https://latex.codecogs.com/svg.image?S_{ImageGeneration}=\frac{1}{2}\left(CLIP_{score}&plus;\frac{200-min(200,X)}{200}\right))

## Подзадача 4 – Image Captioning

Для задачи оценки качества сгенерированных текстовых описаний изображений используются метрики **METEOR** и **CLIP score**. 

**METEOR** – метрика, основанная на анализе n-грамм и ориентированная на использование статистической и точной оценки исходного текста. Данная метрика использует функции сопоставления синонимов вместе с точным соответствием слов. 

Алгоритм сначала проводит выравнивание текста между двумя предложениями – строкой эталонного перевода и строкой входного текста для оценивания. Затем используется несколько этапов установления соответствия между словами машинного перевода и эталонного перевода для сопоставления двух строк:
1. Точное установление соответствия — определяются строки, которые являются идентичными в эталонном и машинном переводе.
2. Установление соответствия основ — проводится стемминг (выделение основы слова), и определяются слова с одинаковым корнем в эталонном и машинном переводе.
3. Установление соответствия синонимов — определяются слова, которые являются синонимами в соответствии с RuWordNet.

Выравнивание — это множество соответствий между n-граммами. На соответствие налагается следующее ограничение: каждый n-грамм в предложении-кандидате должен соответствовать одному или ни одному n-грамму в эталонном предложении. Если есть два выравнивания с тем же количеством совпадений, то выбирается то, которое имеет наименьшее количество пересечений для совпадений. Этапы сравнения с эталонными переводами выполняются последовательно, и на каждом из них ко множеству соответствий добавляются только те n-граммы, которые не имели соответствия на предыдущих этапах. Как только будет пройден последний этап, окончательное значение точности (precision) n-грамм вычисляется по следующей формуле:

![image](https://latex.codecogs.com/svg.image?\text{P}=\frac{m}{w_t},)

где $m$ - количество n-грамм в машинном переводе, которые также были найдены в эталонном переводе, $w_t$ — количество n-грамм в машинном переводе. 

Значение полноты (recall) n-грамм (общий n-грамм для эталонных переводов) вычисляется по следующей формуле:

![image](https://latex.codecogs.com/svg.image?\text{R}=\frac{m}{w_r},)

где $w_r$ — количество n-грамм в эталонном переводе.

В результате METEOR рассчитывается как комбинация точности и полноты, используя формулу гармонического среднего, в которой вес полноты в 9 раз больше веса точности:

![image](https://latex.codecogs.com/svg.image?\text{METEOR}=\frac{10PR}{R&plus;9P}.)

Общее значение метрики METEOR вычисляется как усредненное значение метрики, рассчитанное по всем тестовым примерам для этой задачи.

**CLIP score** – метрика, которая позволяет оценить, насколько текстовое описание соответствует визуальному представлению. Метрика рассчитывается аналогично метрике, использующейся в задаче image generation. 

Финальная метрика для оценки задачи **ImageCaptioning** рассчитывается как среднее значение метрик METEOR и CLIP score.

![image](https://latex.codecogs.com/svg.image?S_{ImageCaptioning}&space;=\frac{1}{2}&space;\cdot&space;(METEOR&plus;CLIP_{score})&space;)

## Подзадача 5 – Visual QA

Для задачи VisualQA используется метрика **METEOR**. METEOR рассчитывается аналогично метрике, описанной в задаче image captioning. Однако добавлены некоторые модификации:
1. Учитывается, в какой пропорции предсказанный численный результат отличается от реального численного результата. Для этого из пары предсказанного и эталонного результата выбирается наименьшее число и делится на большее. Таким образом, если числа совпадают, то метрика для этой пары равна 1, в противном случае метрика вычисляется пропорционально.
2. Осуществляется перевод числительных из текстового формата в числовой: "три" - 3.

![image](https://latex.codecogs.com/svg.image?S_{VisualQA}&space;=&space;1/2&space;\cdot&space;(METEOR&plus;CLIP_{score}).)

## Подзадача 6 – Text Recognition in the Wild

В качестве основной метрики для оценки решений участников используется метрика **1 - NED** (_NED_ - _Normalized Edit Distance_), которая рассчитывается следующим образом:

![image](https://latex.codecogs.com/svg.image?\bg{blue}S_{TRitW}&space;=&space;1&space;-&space;NED&space;=&space;1&space;-&space;\frac{D(s_i,&space;\hat{s}_i)}{max(l_i,\hat{l}_i)},)

где $D(\cdot)$ - расстояние Левенштейна; $s_i$ и $\hat{s}_i$ - предсказанная и истинная строки, соответственно; $l_i$ и $\hat{l}_i$ - длины соответствующих строк.

Метрика по каждой из открытых задач изменяется от 0 до 1, где 0 – наихудшее значение, 1 – наилучшее.

## Метрики для скрытых задач

Для каждой из 6 скрытых задач рассчитывается соответствующая ей скрытая метрика $S_k \in [0,1]$, где 0 – наихудшее значение, а 1 – наилучшее значение метрики, $k \in \\{ Hidden1,\ Hidden2,\ Hidden3,\ Hidden4,\ Hidden5,\ Hidden6 \\}$.
